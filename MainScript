# Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# Setting a random seed for reproducibility
np.random.seed(42)

# Step 1: Generate Synthetic Customer Data
# We'll create synthetic data with features similar to the original dataset (Gender, Age, Annual Income, Spending Score).

# Number of customers
n_customers = 200

# Generating gender data (0 = Male, 1 = Female) with a 50-50 split
gender = np.random.choice([0, 1], size=n_customers)

# Generating age data (normally distributed with mean 35 and standard deviation 10)
age = np.random.normal(35, 10, n_customers).astype(int)
age = np.clip(age, 18, 70)  # Ensuring age is within a reasonable range (18-70)

# Generating annual income (normally distributed with mean 60k, std deviation 20k)
annual_income = np.random.normal(60, 20, n_customers).astype(int)
annual_income = np.clip(annual_income, 15, 135)  # Ensuring income is within a reasonable range (15k-135k)

# Generating spending score (randomly distributed between 1 and 100)
spending_score = np.random.randint(1, 101, size=n_customers)

# Creating a DataFrame to hold the data
df = pd.DataFrame({
    'Gender': gender,
    'Age': age,
    'Annual Income (k$)': annual_income,
    'Spending Score (1-100)': spending_score
})

# Step 2: Exploratory Data Analysis (EDA)
# Describing the data
print(df.describe())

# Visualizing the distribution of Age, Annual Income, and Spending Score
plt.figure(figsize=(15, 5))

plt.subplot(1, 3, 1)
sns.histplot(df['Age'], kde=True, color='skyblue')
plt.title('Age Distribution')

plt.subplot(1, 3, 2)
sns.histplot(df['Annual Income (k$)'], kde=True, color='lightgreen')
plt.title('Annual Income Distribution')

plt.subplot(1, 3, 3)
sns.histplot(df['Spending Score (1-100)'], kde=True, color='salmon')
plt.title('Spending Score Distribution')

plt.tight_layout()
plt.show()

# Visualizing the relationship between Annual Income and Spending Score
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='Age', palette='coolwarm', s=100)
plt.title('Annual Income vs Spending Score (colored by Age)')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

# Step 3: Data Preprocessing
# Scaling the features
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']])

# Step 4: Model 1 - K-Means Clustering
# Using the Elbow Method to determine the optimal number of clusters
inertia = []
cluster_range = range(2, 11)

for k in cluster_range:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_features)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(cluster_range, inertia, marker='o', linestyle='--')
plt.title('Elbow Method for Optimal Clusters (K-Means)')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

# Based on the Elbow method, let's choose 5 clusters (for example)
optimal_clusters = 5
kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)
df['KMeans_Cluster'] = kmeans.fit_predict(scaled_features)

# Visualizing the K-Means clustering results
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='KMeans_Cluster', palette='Set1', s=100)
plt.title('Customer Segmentation using K-Means (k=5)')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

# Step 5: Model 2 - DBSCAN
# Using DBSCAN for clustering, with example parameters
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['DBSCAN_Cluster'] = dbscan.fit_predict(scaled_features)

# Visualizing DBSCAN clustering results
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='DBSCAN_Cluster', palette='Set2', s=100)
plt.title('Customer Segmentation using DBSCAN')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

# Step 6: Model 3 - Hierarchical Clustering
# Performing hierarchical clustering
linkage_matrix = linkage(scaled_features, method='ward')
df['Hierarchical_Cluster'] = fcluster(linkage_matrix, t=5, criterion='maxclust')

# Visualizing hierarchical clustering results
plt.figure(figsize=(8, 6))
sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)', hue='Hierarchical_Cluster', palette='Set3', s=100)
plt.title('Customer Segmentation using Hierarchical Clustering')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

# Displaying the dendrogram
plt.figure(figsize=(12, 8))
dendrogram(linkage_matrix, truncate_mode='level', p=5)
plt.title('Dendrogram (Hierarchical Clustering)')
plt.xlabel('Data Points')
plt.ylabel('Distance')
plt.show()

# Step 7: Model Comparison
# Calculating silhouette scores for K-Means and DBSCAN
kmeans_silhouette = silhouette_score(scaled_features, df['KMeans_Cluster'])
dbscan_silhouette = silhouette_score(scaled_features, df['DBSCAN_Cluster'])

print(f'Silhouette Score for K-Means: {kmeans_silhouette:.2f}')
print(f'Silhouette Score for DBSCAN: {dbscan_silhouette:.2f}')

# Discussion:
# - K-Means clustering performed well with a silhouette score of {kmeans_silhouette:.2f}.
# - DBSCAN had a lower silhouette score, but it's better suited for detecting noise and non-linear clusters.
# - Hierarchical clustering provided a hierarchical structure, useful for understanding the nested nature of clusters.

# Final Note:
# The choice of clustering algorithm depends on the problem at hand:
# - K-Means works best with well-separated, spherical clusters.
# - DBSCAN is useful for finding clusters of varying shapes and densities.
# - Hierarchical clustering helps in understanding the data's hierarchical structure.

# End of script
